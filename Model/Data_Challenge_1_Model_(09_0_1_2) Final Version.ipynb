{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I62TXjQZpDdO"
      },
      "source": [
        "# Introduction:\n",
        "In this template, methods are provided to get you started on the task at hand (please see project description). Please implement your solution in the code cells marked with **TODO**. Most of the other code cells are hidden, feel free to explore and change these. These cells implement a basic pipeline for training your model but you may want to explore more complex procedures. **Make sure you run all cells before trying to implement your own solution!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_jdXV_Cj0fY"
      },
      "source": [
        "# Imports and definitions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huq9l3CmzL3L"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "import numpy as np\n",
        "import requests\n",
        "import io\n",
        "from torch.utils.data import TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from scipy.ndimage import rotate\n",
        "%matplotlib inline\n",
        "from torch.utils.data import Dataset\n",
        "import random\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchsummary import summary\n",
        "from tqdm import tqdm\n",
        "import itertools\n",
        "\n",
        "\n",
        "\n",
        "class BatchSampler():\n",
        "  \"\"\"\n",
        "  Implements an iterable which given a torch dataset and a batch_size\n",
        "  will produce batches of data of that given size. The batches are\n",
        "  returned as tuples in the form (images, labels).\n",
        "  Can produce balanced batches, where each batch will have an equal \n",
        "  amount of samples from each class in the dataset. If your dataset is heavily\n",
        "  imbalanced, this might mean throwing away a lot of samples from \n",
        "  over-represented classes!\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, batch_size, dataset, balanced=False):\n",
        "    self.batch_size = batch_size\n",
        "    self.dataset = dataset\n",
        "    self.balanced = balanced\n",
        "    if self.balanced:\n",
        "      # Counting the ocurrence of the class labels:\n",
        "      unique, counts = np.unique(self.dataset.targets, return_counts=True) \n",
        "      indexes = []\n",
        "      # Sampling an equal amount from each class:\n",
        "      for i in range(len(unique)):\n",
        "        indexes.append(np.random.choice(np.where(self.dataset.targets == i)[0], size=counts.min(), replace=False))\n",
        "      # Setting the indexes we will sample from later:\n",
        "      self.indexes = np.concatenate(indexes)\n",
        "    else:\n",
        "      # Setting the indexes we will sample from later (all indexes):\n",
        "      self.indexes = [i for i in range(len(dataset))]\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return (len(self.indexes) // self.batch_size) + 1\n",
        "  \n",
        "  def shuffle(self):\n",
        "    # We do not need to shuffle if we use the balanced sampling method.\n",
        "    # Shuffling is already done when making the balanced samples.\n",
        "    if not self.balanced:\n",
        "      random.shuffle(self.indexes)\n",
        "    \n",
        "  def __iter__(self):\n",
        "    remaining = False\n",
        "    self.shuffle()\n",
        "    # Go over the datset in steps of 'self.batch_size':\n",
        "    for i in range(0, len(self.indexes), self.batch_size):\n",
        "        imgs, labels = [], []\n",
        "        # If our current batch is larger than the remaining data, we quit:\n",
        "        if i + self.batch_size > len(self.indexes):\n",
        "          remaining = True\n",
        "          break\n",
        "        # If not, we yield a complete batch:\n",
        "        else:\n",
        "          # Getting a list of samples from the dataset, given the indexes we defined:\n",
        "          X_batch = [self.dataset[self.indexes[k]][0] for k in range(i, i + self.batch_size)]\n",
        "          Y_batch = [self.dataset[self.indexes[k]][1] for k in range(i, i + self.batch_size)]\n",
        "          # Stacking all the samples and returning the target labels as a tensor:\n",
        "          yield torch.stack(X_batch).float(), torch.tensor(Y_batch).long()\n",
        "    # If there is still data left that was not a full batch:\n",
        "    if remaining:\n",
        "      # Return the last batch (smaller than batch_size):\n",
        "      X_batch = [self.dataset[self.indexes[k]][0] for k in range(i, len(self.indexes))]\n",
        "      Y_batch = [self.dataset[self.indexes[k]][1] for k in range(i, len(self.indexes))]\n",
        "      yield torch.stack(X_batch).float(), torch.tensor(Y_batch).long()\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "  \"\"\"\n",
        "  Creates a DataSet from numpy arrays while keeping the data \n",
        "  in the more efficient numpy arrays for as long as possible and only\n",
        "  converting to torchtensors when needed (torch tensors are the objects used\n",
        "  to pass the data through the neural network and apply weights).\n",
        "  \"\"\"\n",
        "  \n",
        "  def __init__(self, x, y, transform=None, target_transform=None):\n",
        "    self.targets = y\n",
        "    self.imgs = x\n",
        "    self.transform = transform\n",
        "    self.target_transform = target_transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.targets)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    image = torch.from_numpy(self.imgs[idx] / 255).float()\n",
        "    label = self.targets[idx]\n",
        "    return image, label\n",
        "\n",
        "def load_numpy_arr_from_url(url):\n",
        "    \"\"\"\n",
        "    Loads a numpy array from surfdrive. \n",
        "    \n",
        "    Input:\n",
        "    url: Download link of dataset \n",
        "    \n",
        "    Outputs:\n",
        "    dataset: numpy array with input features or labels\n",
        "    \"\"\"\n",
        "    \n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    return np.load(io.BytesIO(response.content)) \n",
        "\n",
        "\n",
        "class_labels = {0: 'Atelectasis',\n",
        "                1: 'Effusion',\n",
        "                2: 'Infiltration',\n",
        "                3: 'No Finding',\n",
        "                4: 'Nodule',\n",
        "                5: 'Pneumothorax'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2qLHzgiJ-ta"
      },
      "source": [
        "# Downloading the data:\n",
        "The following cells will download the pre-processed X-ray images with their accompanying labels.\n",
        "\n",
        "The download (400 MB) may take a while."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0SUE4M0Jmwv"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# Downloading the labels of each image:\n",
        "train_y = load_numpy_arr_from_url('https://surfdrive.surf.nl/files/index.php/s/i6MvQ8nqoiQ9Tci/download')\n",
        "test_y = load_numpy_arr_from_url('https://surfdrive.surf.nl/files/index.php/s/wLXiOjVAW4AWlXY/download')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFl8GOX_L80K"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# Downloading the images:\n",
        "train_x = load_numpy_arr_from_url('https://surfdrive.surf.nl/files/index.php/s/4rwSf9SYO1ydGtK/download')\n",
        "test_x = load_numpy_arr_from_url('https://surfdrive.surf.nl/files/index.php/s/dvY2LpvFo6dHef0/download')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPvS0ZkfjZdl"
      },
      "source": [
        "\n",
        "\n",
        "## changes on data set\n",
        "\n",
        "### Data Augmentation (not being used in base model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data augentation  \n",
        "\n",
        "# defining transfrmation for data points \n",
        "\n",
        "preprocess_aug = transforms.Compose([\n",
        "\n",
        "    #preprocess takes tensor x\n",
        "    transforms.Lambda(lambda x: x.repeat(1, 3, 1, 1)),\n",
        "    transforms.Resize(299),\n",
        "    transforms.CenterCrop(299),\n",
        "    transforms.RandomResizedCrop(299),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    transforms.RandomResizedCrop((128,128))\n",
        "])"
      ],
      "metadata": {
        "id": "5-TijZlmrRrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#example of transformed data point \n",
        "x_eg = torch.cuda.FloatTensor(test_x[0].reshape(128,128))\n",
        "x_eg = preprocess_aug(x_eg.to('cpu').reshape(128,128))\n",
        "plt.imshow(x_eg[0,1],cmap='gray')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "n1DwKsP2fmUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# processing data points for the two last labels, by the above defined transform\n",
        "\n",
        "set_arr = np.array([[train_x[i],train_y[i]] for i in range(len(train_y))])\n",
        "set_arr = set_arr[set_arr[:,1].argsort()]\n",
        "label_sizes = np.array([np.count_nonzero(train_y==i) for i in class_labels])\n",
        "\n",
        "x_pre_process = [train_x[i] for i in range(sum(label_sizes[0:4]), sum(label_sizes))]\n",
        "y_processed = [train_y[i] for i in range(sum(label_sizes[0:4]), sum(label_sizes))]\n",
        "\n",
        "x_processed=[]\n",
        "for i in x_pre_process:\n",
        "    x = torch.cuda.FloatTensor(i.reshape(128,128)) \n",
        "    x = preprocess_aug(x.to('cpu').reshape(128,128))\n",
        "    x = torch.reshape(x[0,0] , (1, 128, 128))\n",
        "    x_processed.append(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "cpwe5AuJ16uF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QmV9SJnc6vW"
      },
      "outputs": [],
      "source": [
        "# Function used for previous testing of model\n",
        "\n",
        "def sizedSet(set_x, set_y):\n",
        "    \"\"\"Given a data set x this function makes all the labeled (by y) sub sets\n",
        "    the same size as the smallest subset of x by removing the other points \n",
        "    \"\"\"\n",
        "\n",
        "    # working out arrays to be used below \n",
        "\n",
        "    set_arr = np.array([[test_x[i],test_y[i]] for i in range(len(test_y))])\n",
        "    set_arr = set_arr[set_arr[:,1].argsort()]\n",
        "    label_sizes = np.array([np.count_nonzero(test_y==i) for i in class_labels])\n",
        "    size = int(min(label_sizes))\n",
        "\n",
        "\n",
        "    # taking care of indeces for new set\n",
        "\n",
        "    new_arr=np.array([])\n",
        "    index = 0 \n",
        "    for l in range(np.size(label_sizes)):\n",
        "        for i in range(size):\n",
        "            new_arr = np.append(new_arr , [set_arr[index+i,0],set_arr[index+i,1]])\n",
        "        index += label_sizes[l]\n",
        "    \n",
        "\n",
        "    # appending points to new set\n",
        "    new_x=[]\n",
        "    new_y=[]\n",
        "    for i in range(int(np.size(new_arr)/2)):\n",
        "        new_x.append(new_arr[2*i])\n",
        "        new_y.append(new_arr[2*i+1])\n",
        "\n",
        "    new_x = np.array(new_x)\n",
        "    new_y = np.array(new_y)\n",
        "    return new_x, new_y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### comment out to use full data set"
      ],
      "metadata": {
        "id": "0QNKLFYgBVEi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gaeGIlWjP39"
      },
      "outputs": [],
      "source": [
        "#train_x , train_y = sizedSet(train_x, train_y)\n",
        "#test_x , test_y = sizedSet(test_x, test_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validation set split (10% from training set)"
      ],
      "metadata": {
        "id": "PMJV_KfWfy2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# uncomment all to use the Validation set split\n",
        "\n",
        "indexs= [i for i in range(len(train_x))]\n",
        "\n",
        "xval_index=random.sample(indexs, int(len(train_x)*0.1)) #10% used for validation\n",
        "xval=  np.array([train_x[i] for i in xval_index])\n",
        "yval=  np.array([train_y[i] for i in xval_index])\n",
        "\n",
        "\n",
        "\n",
        "train_index= [i for i in indexs if i not in xval_index]\n",
        "train_xx= np.array([train_x[i] for i in train_index])\n",
        "train_yy= np.array([train_y[i] for i in train_index])\n",
        "\n",
        "val_dataset = ImageDataset(xval, yval)  #test\n",
        "\n",
        "# Comment out not to use validation split \n",
        "train_x = train_xx\n",
        "train_y = train_yy\n"
      ],
      "metadata": {
        "id": "pnt19GbBf-JN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNZDXSGYlioO"
      },
      "source": [
        "# Plotting the data distribution:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PS0HVYPcMaYO"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# Plotting the label distribution in train/test set:\n",
        "fig, ax = plt.subplots(ncols=2, figsize=[20,10])\n",
        "\n",
        "unique, counts = np.unique(train_y, return_counts=True) \n",
        "ax[0].bar([class_labels[i] + f'\\n({c})' for i, c in zip(unique, counts)], counts)\n",
        "ax[0].set_title('Number of images per class in train-set')\n",
        "\n",
        "unique, counts = np.unique(test_y, return_counts=True) \n",
        "ax[1].bar([class_labels[i] + f'\\n({c})' for i, c in zip(unique, counts)], counts)\n",
        "ax[1].set_title('Number of images per class in test-set');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XjlCEzWln29"
      },
      "source": [
        "#Plotting some samples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0R0kB9rgJ88t"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# Plotting some images\n",
        "unique_labels = set(class_labels.keys())\n",
        "fig, ax = plt.subplots(ncols=len(unique_labels), figsize=[30,10])\n",
        "\n",
        "for k, label in enumerate(unique_labels):\n",
        "  ind = list(train_y).index(label)\n",
        "  ax[k].imshow(train_x[ind].reshape(128,128), cmap='gray')\n",
        "  ax[k].set_title(f'Class:{class_labels[train_y[ind]]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwoPlextltT7"
      },
      "source": [
        "# Building torch datasets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-kfDytTP9I95"
      },
      "outputs": [],
      "source": [
        "train_dataset = ImageDataset(train_x, train_y)\n",
        "test_dataset = ImageDataset(test_x, test_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADPO8ctCicHs"
      },
      "source": [
        "# Defining our model as a neural network:\n",
        "**TODO** define your own model here, follow the structure as presented in the Pytorch tutorial (or see below as an example)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TkbIbEU-O-0e"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):   \n",
        "    def __init__(self, n_classes):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.cnn_layers = nn.Sequential(\n",
        "            # Defining a 2D convolution layer\n",
        "            nn.Conv2d(1, 256, kernel_size=4, stride=1, padding=2, dilation=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.05, inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=4),\n",
        "            nn.Dropout2d(p=0.5, inplace=True),\n",
        "            # Defining another 2D convolution layer\n",
        "            nn.Conv2d(256, 128, kernel_size=4, stride=1, padding=1, dilation=1, groups=4),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.07, inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, dilation=2),\n",
        "            nn.Dropout2d(p=0.25, inplace=True),\n",
        "            # Defining another 2D convolution layer\n",
        "            nn.Conv2d(128, 16, kernel_size=4, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Dropout2d(p=0.125, inplace=True),\n",
        "        )\n",
        "\n",
        "        self.linear_layers = nn.Sequential(\n",
        "            nn.Linear(144, 256),\n",
        "            nn.Linear(256,512),\n",
        "            nn.Dropout(p=0.5, inplace=True),\n",
        "            nn.Linear(512,512),\n",
        "            nn.Dropout(p=0.25, inplace=True),\n",
        "            nn.Linear(512,256),\n",
        "            nn.Dropout(p=0.125, inplace=True),\n",
        "            nn.Linear(256,64),\n",
        "            nn.Linear(64, n_classes)\n",
        "        )\n",
        "\n",
        "    # Defining the forward pass    \n",
        "    def forward(self, x):\n",
        "        x = self.cnn_layers(x)\n",
        "        # After our convolutional layers which are 2D, we need to flatten our\n",
        "        # input to be 1 dimensional, as the linear layers require this.\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.linear_layers(x)\n",
        "        return x\n",
        "\n",
        "# Make sure your model instance is assigned to a variable 'model':\n",
        "model = Net(n_classes = 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvkln77gmrOl"
      },
      "source": [
        "#Moving model to CUDA, verifying model structure and printing a summary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5o5m_hEocYY"
      },
      "outputs": [],
      "source": [
        "# IMPORTANT! Set this to True to see actual errors regarding \n",
        "# the structure of your model (CUDA hides them)!\n",
        "# Also make sure you set this to False again for actual model training\n",
        "# as training your model with GPU-acceleration (CUDA) is much faster.\n",
        "DEBUG = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJj3tmq8mnY-"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# Moving our model to the right device (CUDA will speed training up significantly!)\n",
        "if torch.cuda.is_available() and not DEBUG:\n",
        "  device = 'cuda'\n",
        "  print(f\"Device = {device}\")\n",
        "  model.to(device)\n",
        "  # Creating a summary of our model and its layers:\n",
        "  summary(model, (1, 128, 128), device=device)\n",
        "else:\n",
        "  device='cpu'\n",
        "  print(f\"Device = {device}\")\n",
        "  # Creating a summary of our model and its layers:\n",
        "  summary(model, (1, 128, 128), device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1Yjb3uzm_ar"
      },
      "source": [
        "# Defining our loss and optimizer functions:\n",
        "**TODO** Please define your own optimizer and loss function."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bellow a lot of optimization and loss functions are commented out,\n",
        "#### these are the different trials"
      ],
      "metadata": {
        "id": "MaMIF2anBjVj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gh6efXnHm-SF"
      },
      "outputs": [],
      "source": [
        "# computing ratios that are used in the loss function, further explanation in report \n",
        "\n",
        "label_sizes = np.array([np.count_nonzero(train_y==i) for i in class_labels])\n",
        "label_ratios = [(1-i/sum(label_sizes)) for i in label_sizes]\n",
        "label_ratios_l = [float(i/sum(label_ratios)) for i in label_ratios]\n",
        "\n",
        "#optimizer = optim.SGD(model.parameters(), lr=0.0075, momentum=0.125)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.9999), eps=1e-08, weight_decay=0.001, amsgrad=True)\n",
        "#optimizer = optim.Adadelta(model.parameters(), lr=1.0, rho=0.9, eps=1e-06, weight_decay=0)\n",
        "#optimizer = optim.RMSprop(model.parameters(), lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=True)\n",
        "#optimizer = optim.NAdam(model.parameters(), lr=0.002, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, momentum_decay=0.004)\n",
        "loss_function = nn.CrossEntropyLoss(weight=torch.tensor(label_ratios_l).to(device))\n",
        "#loss_function = nn.MultiMarginLoss(p=1, margin=1.0, weight=torch.tensor(label_ratios_l).to(device), size_average=None, reduce=None, reduction='mean')\n",
        "#loss_function = nn.NLLLoss(weight=torch.tensor(label_ratios_l).to(device), size_average=None, ignore_index=- 100, reduce=True, reduction='sum')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "id1Np1S3nNoh"
      },
      "source": [
        "#Defining our training/testing methods:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iz7CfpfYnM_b"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "def train_model(model, train_sampler, optimizer, loss_function):\n",
        "  # Lets keep track of all the losses:\n",
        "  losses = []\n",
        "  # Put the model in train mode:\n",
        "  model.train()\n",
        "  # Feed all the batches one by one:\n",
        "  for batch in tqdm(train_sampler):\n",
        "    # Get a batch:\n",
        "    x, y = batch\n",
        "    # Making sure our samples are stored on the same device as our model:\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    # Get predictions:\n",
        "    predictions = model.forward(x)\n",
        "    loss = loss_function(predictions, y)\n",
        "    losses.append(loss)\n",
        "    # We first need to make sure we reset our optimizer at the start.\n",
        "    # We want to learn from each batch seperately, \n",
        "    # not from the entire dataset at once.\n",
        "    optimizer.zero_grad()\n",
        "    # We now backpropagate our loss through our model:\n",
        "    loss.backward()\n",
        "    # We then make the optimizer take a step in the right direction.\n",
        "    optimizer.step()\n",
        "  return losses\n",
        "\n",
        "def test_model(model, test_sampler, loss_function):\n",
        "  # Setting the model to evaluation mode:\n",
        "  model.eval()\n",
        "  losses = []\n",
        "  # We need to make sure we do not update our model based on the test data:\n",
        "  with torch.no_grad():\n",
        "    for (x, y) in tqdm(test_sampler):\n",
        "      # Making sure our samples are stored on the same device as our model:\n",
        "      x = x.to(device)\n",
        "      y = y.to(device)\n",
        "      prediction = model.forward(x)\n",
        "      loss = loss_function(prediction, y)\n",
        "      losses.append(loss)\n",
        "  return losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDG8XNh_ojrW"
      },
      "source": [
        "#Training our model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5mPvRt5op0z"
      },
      "outputs": [],
      "source": [
        "n_epochs = 35\n",
        "batch_size = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAunFnrpSDKU"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# Lets now train and test our model for multiple epochs:\n",
        "train_sampler = BatchSampler(batch_size=batch_size, dataset=train_dataset, balanced=False)\n",
        "test_sampler = BatchSampler(batch_size=100, dataset=test_dataset, balanced=False)\n",
        "\n",
        "mean_losses_train = []\n",
        "mean_losses_test = []\n",
        "accuracies = []\n",
        "for e in range(n_epochs):\n",
        "  # Training:\n",
        "  losses = train_model(model, train_sampler, optimizer, loss_function)\n",
        "  # Calculating and printing statistics:\n",
        "  mean_loss = sum(losses) / len(losses)\n",
        "  mean_losses_train.append(mean_loss)\n",
        "  print(f'\\nEpoch {e + 1} training done, loss on train set: {mean_loss}\\n')\n",
        "\n",
        "  # Testing:\n",
        "  losses = test_model(model, test_sampler, loss_function)\n",
        "  # Calculating and printing statistics:\n",
        "  mean_loss = sum(losses) / len(losses)\n",
        "  mean_losses_test.append(mean_loss)\n",
        "  print(f'\\nEpoch {e + 1} testing done, loss on test set: {mean_loss}\\n')\n",
        "  # Plotting the historic loss:\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.plot(mean_losses_train, label='Train loss')\n",
        "  ax.plot(mean_losses_test, label='Test loss')\n",
        "  ax.legend()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tnZQoGJqKND"
      },
      "source": [
        "# Evaluation our model:\n",
        "**TODO** write your own methods to evaluate the model. For example, calculate the accuracy of the model on the test-set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jw8ox5pYqWaD"
      },
      "outputs": [],
      "source": [
        "# Accuracy for a theoretical dummy model, which evaluates each point to a random label\n",
        "\n",
        "# calculated accuracy with same method as used to evaluate the model\n",
        "dummy_accuracy=[i/sum(label_sizes) for i in label_sizes]\n",
        "label_ratios = [(1-i/sum(label_sizes)) for i in label_sizes]\n",
        "label_ratios_l = [i/sum(label_ratios) for i in label_ratios]\n",
        "final_dummy_accuracy=0\n",
        "for i in range(6):\n",
        "    final_dummy_accuracy+= dummy_accuracy[i]*label_ratios_l[i]\n",
        "\n",
        "# =======================================>\n",
        "\n",
        "# Computing confusion matrix of model on inputed set \n",
        "\n",
        "# Negative -> Healthy\n",
        "# Positive -> Sick\n",
        "\n",
        "def ConfusionMatrix(testing_x,testing_y):\n",
        "    \"\"\" False positives + False Negatives\n",
        "Will the no-finding in a set and the non-no-finding on another\n",
        "    \"\"\"\n",
        "    \n",
        "    # prepare data \n",
        "    testing_arr = np.array([[testing_x[i],testing_y[i]] for i in range(len(testing_y))])\n",
        "    filter_arr = [i[1]==3 for i in testing_arr]\n",
        "    healthy_arr = testing_arr[filter_arr]\n",
        "    filter_arr = [not i for i in filter_arr]\n",
        "    disease_arr = testing_arr[filter_arr]\n",
        "\n",
        "    # prepare sampler data sets \n",
        "    pre_x = [i[0] for i in healthy_arr]\n",
        "    pre_y = [i[1] for i in healthy_arr]\n",
        "    healthy_len = len(pre_y)\n",
        "    test_dataset_healthy = ImageDataset(pre_x,pre_y)\n",
        "    test_sampler_healthy = BatchSampler(batch_size=100, dataset=test_dataset_healthy, balanced=False)\n",
        "\n",
        "    pre_x = [i[0] for i in disease_arr]\n",
        "    pre_y = [i[1] for i in disease_arr]\n",
        "    disease_len = len(pre_y)\n",
        "    test_dataset_disease = ImageDataset(pre_x,pre_y)\n",
        "    test_sampler_disease = BatchSampler(batch_size=100, dataset=test_dataset_disease, balanced=False)\n",
        "\n",
        "\n",
        "    # == eval TP and FP \n",
        "\n",
        "    model.eval()\n",
        "    # We need to make sure we do not update our model based on the test data:\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        count = 0\n",
        "        for (x, y) in tqdm(test_sampler_disease):\n",
        "        # Making sure our samples are stored on the same device as our model:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            prediction = model.forward(x).argmax(axis=1)\n",
        "            correct += sum(prediction == y)\n",
        "            \n",
        "            count += len(y)\n",
        "        true_pos = (correct).detach().cpu().numpy()\n",
        "        false_pos = (disease_len-correct).detach().cpu().numpy()\n",
        "\n",
        "\n",
        "    # == eval TN and FN\n",
        "\n",
        "    # We need to make sure we do not update our model based on the test data:\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        count = 0\n",
        "        for (x, y) in tqdm(test_sampler_healthy):\n",
        "        # Making sure our samples are stored on the same device as our model:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            prediction = model.forward(x).argmax(axis=1)\n",
        "            correct += sum(prediction == y)\n",
        "            \n",
        "            count += len(y)\n",
        "        true_neg = (correct).detach().cpu().numpy()\n",
        "        false_neg = (healthy_len-correct).detach().cpu().numpy()\n",
        "\n",
        "\n",
        "\n",
        "    return true_pos, false_pos, true_neg, false_neg\n",
        "\n",
        "# =======================================>\n",
        "\n",
        "# Evaluation method 0.1.1\n",
        "\n",
        "def evalMethod011(testing_x,testing_y):\n",
        "    \"\"\" Evaluation method 0.1.1\n",
        "Takes test set and separates points in 6 sub sets per label in the y component\n",
        "accuracy of the model is taken to each subset by calculating the percentage of \n",
        "points which were classified correctly.\n",
        "A final accuracy is calculating by a linear combination of these sub-acuracies\n",
        "weighted by the size of each sub data set (|n_i|), the higher the size the \n",
        "smaller the weight. \n",
        "    \"\"\"\n",
        "    testing_arr = np.array([[testing_x[i],testing_y[i]] for i in range(len(testing_y))])\n",
        "    testing_arr = testing_arr[testing_arr[:,1].argsort()]\n",
        "    label_sizes = np.array([np.count_nonzero(testing_y==i) for i in class_labels])\n",
        "    testing_x_l = np.array([testing_arr[i,0] for i in range(len(testing_y))])\n",
        "    testing_y_l = np.array([testing_arr[i,1] for i in range(len(testing_y))])\n",
        "\n",
        "    model.eval()\n",
        "    # We need to make sure we do not update our model based on the test data:\n",
        "    a=0\n",
        "    label_index=[]\n",
        "    for i in range(len(label_sizes)):\n",
        "        a+=label_sizes[i]\n",
        "        label_index.append(a)\n",
        "\n",
        "    p_index=0\n",
        "\n",
        "    accuracy_l=[]\n",
        "\n",
        "    for l in class_labels: \n",
        "        pre_x=[]\n",
        "        pre_y=[]\n",
        "        for i in range(p_index,label_index[l]):\n",
        "            pre_x.append(testing_x_l[i])\n",
        "            pre_y.append(testing_y_l[i])\n",
        "\n",
        "        p_index = label_index[l]   \n",
        "\n",
        "        test_dataset_l = ImageDataset(pre_x,pre_y)\n",
        "        test_sampler_l = BatchSampler(batch_size=100, dataset=test_dataset_l, balanced=False)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            correct = 0\n",
        "            count = 0\n",
        "            for (x, y) in tqdm(test_sampler_l):\n",
        "                # Making sure our samples are stored on the same device as our model:\n",
        "                x = x.to(device)\n",
        "                y = y.to(device)\n",
        "                prediction = model.forward(x).argmax(axis=1)\n",
        "                correct += sum(prediction == y)\n",
        "                count += len(y)\n",
        "        accuracy = (correct/count).detach().cpu().numpy()\n",
        "        accuracy_l.append(accuracy)\n",
        "        print(f'\\nAccuracy of model on test set for label {class_labels[l]}: {accuracy}')\n",
        "\n",
        "    accuracy_l=[i for i in accuracy_l]\n",
        "    label_ratios = [(1-i/sum(label_sizes)) for i in label_sizes]\n",
        "    label_ratios_l = [i/sum(label_ratios) for i in label_ratios]\n",
        "    final_accuracy=0\n",
        "    for i in range(6):\n",
        "        final_accuracy+= accuracy_l[i]*label_ratios_l[i]\n",
        "    print('\\n')\n",
        "    print(f\"The general accurcy of the model is: {final_accuracy}\")\n",
        "    print('\\n')\n",
        "    print(f\"Dummy model accuracy = {final_dummy_accuracy}\")\n",
        "\n",
        "# ==========================================>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_gmbaXr42VA"
      },
      "outputs": [],
      "source": [
        "TP, FP, TN, FN = ConfusionMatrix(test_x,test_y)\n",
        "\n",
        "Precision = TP/(TP+FP)\n",
        "Recall = TP/(TP+FN)\n",
        "print(\"\\n\")\n",
        "print(\"Negative -> Healthy\")\n",
        "print(\"Positive -> Sick\")\n",
        "print(\"\\n\")\n",
        "print(f\"TP = {TP}\")\n",
        "print(f\"TN = {TN}\")\n",
        "print(f\"FP = {FP}\")\n",
        "print(f\"FN = {FN}\")\n",
        "print(f\"Precision = {Precision}\")\n",
        "print(f\"Recall = {Recall}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mj2py6-v8iXh"
      },
      "outputs": [],
      "source": [
        "print(\"Evaluation on test set: \")\n",
        "evalMethod011(test_x,test_y)\n",
        "\n",
        "print('\\n\\n')\n",
        "\n",
        "print(\"Evaluation on training set:\")\n",
        "evalMethod011(train_x,train_y)\n",
        "\n",
        "print('\\n\\n')\n",
        "\n",
        "print(\"Evaluation on validation set:\")\n",
        "evalMethod011(xval, yval)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1fo7K_1mq1L"
      },
      "source": [
        "# Saving our model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esVBn4-N1ij9"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#\n",
        "#drive.mount('/content/gdrive')\n",
        "#torch.save(model.state_dict(), '/content/gdrive/My Drive/weights_model.txt')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Data_Challenge_1_Model_(07_0_1_2).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}