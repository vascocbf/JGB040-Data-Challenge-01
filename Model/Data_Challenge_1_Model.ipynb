{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I62TXjQZpDdO"
      },
      "source": [
        "# Introduction:\n",
        "In this template, methods are provided to get you started on the task at hand (please see project description). Please implement your solution in the code cells marked with **TODO**. Most of the other code cells are hidden, feel free to explore and change these. These cells implement a basic pipeline for training your model but you may want to explore more complex procedures. **Make sure you run all cells before trying to implement your own solution!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_jdXV_Cj0fY"
      },
      "source": [
        "# Imports and definitions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huq9l3CmzL3L"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "import numpy as np\n",
        "import requests\n",
        "import io\n",
        "from torch.utils.data import TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from scipy.ndimage import rotate\n",
        "%matplotlib inline\n",
        "from torch.utils.data import Dataset\n",
        "import random\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchsummary import summary\n",
        "from tqdm import tqdm\n",
        "\n",
        "class BatchSampler():\n",
        "  \"\"\"\n",
        "  Implements an iterable which given a torch dataset and a batch_size\n",
        "  will produce batches of data of that given size. The batches are\n",
        "  returned as tuples in the form (images, labels).\n",
        "  Can produce balanced batches, where each batch will have an equal \n",
        "  amount of samples from each class in the dataset. If your dataset is heavily\n",
        "  imbalanced, this might mean throwing away a lot of samples from \n",
        "  over-represented classes!\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, batch_size, dataset, balanced=False):\n",
        "    self.batch_size = batch_size\n",
        "    self.dataset = dataset\n",
        "    self.balanced = balanced\n",
        "    if self.balanced:\n",
        "      # Counting the ocurrence of the class labels:\n",
        "      unique, counts = np.unique(self.dataset.targets, return_counts=True) \n",
        "      indexes = []\n",
        "      # Sampling an equal amount from each class:\n",
        "      for i in range(len(unique)):\n",
        "        indexes.append(np.random.choice(np.where(self.dataset.targets == i)[0], size=counts.min(), replace=False))\n",
        "      # Setting the indexes we will sample from later:\n",
        "      self.indexes = np.concatenate(indexes)\n",
        "    else:\n",
        "      # Setting the indexes we will sample from later (all indexes):\n",
        "      self.indexes = [i for i in range(len(dataset))]\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return (len(self.indexes) // self.batch_size) + 1\n",
        "  \n",
        "  def shuffle(self):\n",
        "    # We do not need to shuffle if we use the balanced sampling method.\n",
        "    # Shuffling is already done when making the balanced samples.\n",
        "    if not self.balanced:\n",
        "      random.shuffle(self.indexes)\n",
        "    \n",
        "  def __iter__(self):\n",
        "    remaining = False\n",
        "    self.shuffle()\n",
        "    # Go over the datset in steps of 'self.batch_size':\n",
        "    for i in range(0, len(self.indexes), self.batch_size):\n",
        "        imgs, labels = [], []\n",
        "        # If our current batch is larger than the remaining data, we quit:\n",
        "        if i + self.batch_size > len(self.indexes):\n",
        "          remaining = True\n",
        "          break\n",
        "        # If not, we yield a complete batch:\n",
        "        else:\n",
        "          # Getting a list of samples from the dataset, given the indexes we defined:\n",
        "          X_batch = [self.dataset[self.indexes[k]][0] for k in range(i, i + self.batch_size)]\n",
        "          Y_batch = [self.dataset[self.indexes[k]][1] for k in range(i, i + self.batch_size)]\n",
        "          # Stacking all the samples and returning the target labels as a tensor:\n",
        "          yield torch.stack(X_batch).float(), torch.tensor(Y_batch).long()\n",
        "    # If there is still data left that was not a full batch:\n",
        "    if remaining:\n",
        "      # Return the last batch (smaller than batch_size):\n",
        "      X_batch = [self.dataset[self.indexes[k]][0] for k in range(i, len(self.indexes))]\n",
        "      Y_batch = [self.dataset[self.indexes[k]][1] for k in range(i, len(self.indexes))]\n",
        "      yield torch.stack(X_batch).float(), torch.tensor(Y_batch).long()\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "  \"\"\"\n",
        "  Creates a DataSet from numpy arrays while keeping the data \n",
        "  in the more efficient numpy arrays for as long as possible and only\n",
        "  converting to torchtensors when needed (torch tensors are the objects used\n",
        "  to pass the data through the neural network and apply weights).\n",
        "  \"\"\"\n",
        "  \n",
        "  def __init__(self, x, y, transform=None, target_transform=None):\n",
        "    self.targets = y\n",
        "    self.imgs = x\n",
        "    self.transform = transform\n",
        "    self.target_transform = target_transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.targets)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    image = torch.from_numpy(self.imgs[idx] / 255).float()\n",
        "    label = self.targets[idx]\n",
        "    return image, label\n",
        "\n",
        "def load_numpy_arr_from_url(url):\n",
        "    \"\"\"\n",
        "    Loads a numpy array from surfdrive. \n",
        "    \n",
        "    Input:\n",
        "    url: Download link of dataset \n",
        "    \n",
        "    Outputs:\n",
        "    dataset: numpy array with input features or labels\n",
        "    \"\"\"\n",
        "    \n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    return np.load(io.BytesIO(response.content)) \n",
        "\n",
        "\n",
        "class_labels = {0: 'Atelectasis',\n",
        "                1: 'Effusion',\n",
        "                2: 'Infiltration',\n",
        "                3: 'No Finding',\n",
        "                4: 'Nodule',\n",
        "                5: 'Pneumothorax'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2qLHzgiJ-ta"
      },
      "source": [
        "# Downloading the data:\n",
        "The following cells will download the pre-processed X-ray images with their accompanying labels.\n",
        "\n",
        "The download (400 MB) may take a while."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0SUE4M0Jmwv"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# Downloading the labels of each image:\n",
        "train_y = load_numpy_arr_from_url('https://surfdrive.surf.nl/files/index.php/s/i6MvQ8nqoiQ9Tci/download')\n",
        "test_y = load_numpy_arr_from_url('https://surfdrive.surf.nl/files/index.php/s/wLXiOjVAW4AWlXY/download')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFl8GOX_L80K"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# Downloading the images:\n",
        "train_x = load_numpy_arr_from_url('https://surfdrive.surf.nl/files/index.php/s/4rwSf9SYO1ydGtK/download')\n",
        "test_x = load_numpy_arr_from_url('https://surfdrive.surf.nl/files/index.php/s/dvY2LpvFo6dHef0/download')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNZDXSGYlioO"
      },
      "source": [
        "# Plotting the data distribution:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "PS0HVYPcMaYO",
        "outputId": "223fd8c6-510f-4c0e-99cb-839e80ee16e2"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# Plotting the label distribution in train/test set:\n",
        "fig, ax = plt.subplots(ncols=2, figsize=[20,10])\n",
        "\n",
        "unique, counts = np.unique(train_y, return_counts=True) \n",
        "ax[0].barh([class_labels[i] + f'\\n({c})' for i, c in zip(unique, counts)], counts)\n",
        "ax[0].set_title('Number of images per class in train-set')\n",
        "\n",
        "unique, counts = np.unique(test_y, return_counts=True) \n",
        "ax[1].barh([class_labels[i] + f'\\n({c})' for i, c in zip(unique, counts)], counts)\n",
        "ax[1].set_title('Number of images per class in test-set');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XjlCEzWln29"
      },
      "source": [
        "#Plotting some samples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "0R0kB9rgJ88t",
        "outputId": "45ee4835-567f-430f-c2fe-df1b53a80c30"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# Plotting some images\n",
        "unique_labels = set(class_labels.keys())\n",
        "fig, ax = plt.subplots(ncols=len(unique_labels), figsize=[30,10])\n",
        "\n",
        "for k, label in enumerate(unique_labels):\n",
        "  ind = list(train_y).index(label)\n",
        "  ax[k].imshow(train_x[ind].reshape(128,128), cmap='gray')\n",
        "  ax[k].set_title(f'Class:{class_labels[train_y[ind]]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwoPlextltT7"
      },
      "source": [
        "# Building torch datasets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-kfDytTP9I95"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "train_dataset = ImageDataset(train_x, train_y)\n",
        "test_dataset = ImageDataset(test_x, test_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADPO8ctCicHs"
      },
      "source": [
        "# Defining our model as a neural network:\n",
        "**TODO** define your own model here, follow the structure as presented in the Pytorch tutorial (or see below as an example)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TkbIbEU-O-0e"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):   \n",
        "    def __init__(self, n_classes):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.cnn_layers = nn.Sequential(\n",
        "            # Defining a 2D convolution layer\n",
        "            nn.Conv2d(1, 64, kernel_size=4, stride=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=4),\n",
        "            torch.nn.Dropout(p=0.5, inplace=True),\n",
        "            # Defining another 2D convolution layer\n",
        "            nn.Conv2d(64, 32, kernel_size=4, stride=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3),\n",
        "            torch.nn.Dropout(p=0.25, inplace=True),\n",
        "            # Defining another 2D convolution layer\n",
        "            nn.Conv2d(32, 16, kernel_size=4, stride=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            torch.nn.Dropout(p=0.125, inplace=True),\n",
        "        )\n",
        "\n",
        "        self.linear_layers = nn.Sequential(\n",
        "            nn.Linear(144, 256),\n",
        "            nn.Linear(256, n_classes)\n",
        "        )\n",
        "\n",
        "    # Defining the forward pass    \n",
        "    def forward(self, x):\n",
        "        x = self.cnn_layers(x)\n",
        "        # After our convolutional layers which are 2D, we need to flatten our\n",
        "        # input to be 1 dimensional, as the linear layers require this.\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.linear_layers(x)\n",
        "        return x\n",
        "\n",
        "# Make sure your model instance is assigned to a variable 'model':\n",
        "model = Net(n_classes = 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1Yjb3uzm_ar"
      },
      "source": [
        "# Defining our loss and optimizer functions:\n",
        "**TODO** Please define your own optimizer and loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gh6efXnHm-SF"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.1)\n",
        "loss_function = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvkln77gmrOl"
      },
      "source": [
        "#Moving model to CUDA, verifying model structure and printing a summary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5o5m_hEocYY"
      },
      "outputs": [],
      "source": [
        "# IMPORTANT! Set this to True to see actual errors regarding \n",
        "# the structure of your model (CUDA hides them)!\n",
        "# Also make sure you set this to False again for actual model training\n",
        "# as training your model with GPU-acceleration (CUDA) is much faster.\n",
        "DEBUG = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJj3tmq8mnY-",
        "outputId": "b224500f-39b7-42a5-fbe5-2797847d64e8"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# Moving our model to the right device (CUDA will speed training up significantly!)\n",
        "if torch.cuda.is_available() and not DEBUG:\n",
        "  device = 'cuda'\n",
        "  model.to(device)\n",
        "  # Creating a summary of our model and its layers:\n",
        "  summary(model, (1, 128, 128), device=device)\n",
        "else:\n",
        "  device='cpu'\n",
        "  # Creating a summary of our model and its layers:\n",
        "  summary(model, (1, 128, 128), device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "id1Np1S3nNoh"
      },
      "source": [
        "#Defining our training/testing methods:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iz7CfpfYnM_b"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "def train_model(model, train_sampler, optimizer, loss_function):\n",
        "  # Lets keep track of all the losses:\n",
        "  losses = []\n",
        "  # Put the model in train mode:\n",
        "  model.train()\n",
        "  # Feed all the batches one by one:\n",
        "  for batch in tqdm(train_sampler):\n",
        "    # Get a batch:\n",
        "    x, y = batch\n",
        "    # Making sure our samples are stored on the same device as our model:\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    # Get predictions:\n",
        "    predictions = model.forward(x)\n",
        "    loss = loss_function(predictions, y)\n",
        "    losses.append(loss)\n",
        "    # We first need to make sure we reset our optimizer at the start.\n",
        "    # We want to learn from each batch seperately, \n",
        "    # not from the entire dataset at once.\n",
        "    optimizer.zero_grad()\n",
        "    # We now backpropagate our loss through our model:\n",
        "    loss.backward()\n",
        "    # We then make the optimizer take a step in the right direction.\n",
        "    optimizer.step()\n",
        "  return losses\n",
        "\n",
        "def test_model(model, test_sampler, loss_function):\n",
        "  # Setting the model to evaluation mode:\n",
        "  model.eval()\n",
        "  losses = []\n",
        "  # We need to make sure we do not update our model based on the test data:\n",
        "  with torch.no_grad():\n",
        "    for (x, y) in tqdm(test_sampler):\n",
        "      # Making sure our samples are stored on the same device as our model:\n",
        "      x = x.to(device)\n",
        "      y = y.to(device)\n",
        "      prediction = model.forward(x)\n",
        "      loss = loss_function(prediction, y)\n",
        "      losses.append(loss)\n",
        "  return losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDG8XNh_ojrW"
      },
      "source": [
        "#Training our model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5mPvRt5op0z"
      },
      "outputs": [],
      "source": [
        "n_epochs = 10\n",
        "batch_size = 25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uAunFnrpSDKU",
        "outputId": "32ae4437-fc2e-46f1-af63-b5c6dde3ad99"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# Lets now train and test our model for multiple epochs:\n",
        "train_sampler = BatchSampler(batch_size=batch_size, dataset=train_dataset, balanced=False)\n",
        "test_sampler = BatchSampler(batch_size=100, dataset=test_dataset, balanced=False)\n",
        "\n",
        "mean_losses_train = []\n",
        "mean_losses_test = []\n",
        "accuracies = []\n",
        "for e in range(n_epochs):\n",
        "  # Training:\n",
        "  losses = train_model(model, train_sampler, optimizer, loss_function)\n",
        "  # Calculating and printing statistics:\n",
        "  mean_loss = sum(losses) / len(losses)\n",
        "  mean_losses_train.append(mean_loss)\n",
        "  print(f'\\nEpoch {e + 1} training done, loss on train set: {mean_loss}\\n')\n",
        "\n",
        "  # Testing:\n",
        "  losses = test_model(model, test_sampler, loss_function)\n",
        "  # Calculating and printing statistics:\n",
        "  mean_loss = sum(losses) / len(losses)\n",
        "  mean_losses_test.append(mean_loss)\n",
        "  print(f'\\nEpoch {e + 1} testing done, loss on test set: {mean_loss}\\n')\n",
        "  # Plotting the historic loss:\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.plot(mean_losses_train, label='Train loss')\n",
        "  ax.plot(mean_losses_test, label='Test loss')\n",
        "  ax.legend()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tnZQoGJqKND"
      },
      "source": [
        "# Evaluation our model:\n",
        "**TODO** write your own methods to evaluate the model. For example, calculate the accuracy of the model on the test-set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jw8ox5pYqWaD",
        "outputId": "375579c8-77bb-45d2-dda9-19f249da6a7f"
      },
      "outputs": [],
      "source": [
        "evalutation_method = \"0.1.0\"\n",
        "\n",
        "model.eval()\n",
        "# We need to make sure we do not update our model based on the test data:\n",
        "with torch.no_grad():\n",
        "  correct = 0\n",
        "  count = 0\n",
        "  for (x, y) in tqdm(test_sampler):\n",
        "    # Making sure our samples are stored on the same device as our model:\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    prediction = model.forward(x).argmax(axis=1)\n",
        "    correct += sum(prediction == y)\n",
        "    count += len(y)\n",
        "accuracy = (correct/count).detach().cpu().numpy()\n",
        "print(f'\\nAccuracy of model on test set: {accuracy}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3nM958rnYPH"
      },
      "source": [
        "This accuracy isn't great. Your task is to find a better model that performs better at the classification task. Other methods of evaluation might tell you more why a particular model is not performing well (accuracy is a quite limited aggregated performance metric). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def LogModel():\n",
        "    # Makes a quick spreadsheet file to log the various model iterations made \n",
        "    mode_accuracy = accuracy\n",
        "    \n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1fo7K_1mq1L"
      },
      "source": [
        "# Saving our model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esVBn4-N1ij9"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "torch.save(model.state_dict(), '/content/gdrive/My Drive/weights_model.txt')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Data Challenge 1 Template Final",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
