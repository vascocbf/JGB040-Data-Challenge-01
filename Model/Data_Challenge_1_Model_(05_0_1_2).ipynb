{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I62TXjQZpDdO"
      },
      "source": [
        "# Introduction:\n",
        "In this template, methods are provided to get you started on the task at hand (please see project description). Please implement your solution in the code cells marked with **TODO**. Most of the other code cells are hidden, feel free to explore and change these. These cells implement a basic pipeline for training your model but you may want to explore more complex procedures. **Make sure you run all cells before trying to implement your own solution!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_jdXV_Cj0fY"
      },
      "source": [
        "# Imports and definitions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "huq9l3CmzL3L"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "import numpy as np\n",
        "import requests\n",
        "import io\n",
        "from torch.utils.data import TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from scipy.ndimage import rotate\n",
        "%matplotlib inline\n",
        "from torch.utils.data import Dataset\n",
        "import random\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchsummary import summary\n",
        "from tqdm import tqdm\n",
        "import itertools\n",
        "\n",
        "\n",
        "class BatchSampler():\n",
        "  \"\"\"\n",
        "  Implements an iterable which given a torch dataset and a batch_size\n",
        "  will produce batches of data of that given size. The batches are\n",
        "  returned as tuples in the form (images, labels).\n",
        "  Can produce balanced batches, where each batch will have an equal \n",
        "  amount of samples from each class in the dataset. If your dataset is heavily\n",
        "  imbalanced, this might mean throwing away a lot of samples from \n",
        "  over-represented classes!\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, batch_size, dataset, balanced=False):\n",
        "    self.batch_size = batch_size\n",
        "    self.dataset = dataset\n",
        "    self.balanced = balanced\n",
        "    if self.balanced:\n",
        "      # Counting the ocurrence of the class labels:\n",
        "      unique, counts = np.unique(self.dataset.targets, return_counts=True) \n",
        "      indexes = []\n",
        "      # Sampling an equal amount from each class:\n",
        "      for i in range(len(unique)):\n",
        "        indexes.append(np.random.choice(np.where(self.dataset.targets == i)[0], size=counts.min(), replace=False))\n",
        "      # Setting the indexes we will sample from later:\n",
        "      self.indexes = np.concatenate(indexes)\n",
        "    else:\n",
        "      # Setting the indexes we will sample from later (all indexes):\n",
        "      self.indexes = [i for i in range(len(dataset))]\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return (len(self.indexes) // self.batch_size) + 1\n",
        "  \n",
        "  def shuffle(self):\n",
        "    # We do not need to shuffle if we use the balanced sampling method.\n",
        "    # Shuffling is already done when making the balanced samples.\n",
        "    if not self.balanced:\n",
        "      random.shuffle(self.indexes)\n",
        "    \n",
        "  def __iter__(self):\n",
        "    remaining = False\n",
        "    self.shuffle()\n",
        "    # Go over the datset in steps of 'self.batch_size':\n",
        "    for i in range(0, len(self.indexes), self.batch_size):\n",
        "        imgs, labels = [], []\n",
        "        # If our current batch is larger than the remaining data, we quit:\n",
        "        if i + self.batch_size > len(self.indexes):\n",
        "          remaining = True\n",
        "          break\n",
        "        # If not, we yield a complete batch:\n",
        "        else:\n",
        "          # Getting a list of samples from the dataset, given the indexes we defined:\n",
        "          X_batch = [self.dataset[self.indexes[k]][0] for k in range(i, i + self.batch_size)]\n",
        "          Y_batch = [self.dataset[self.indexes[k]][1] for k in range(i, i + self.batch_size)]\n",
        "          # Stacking all the samples and returning the target labels as a tensor:\n",
        "          yield torch.stack(X_batch).float(), torch.tensor(Y_batch).long()\n",
        "    # If there is still data left that was not a full batch:\n",
        "    if remaining:\n",
        "      # Return the last batch (smaller than batch_size):\n",
        "      X_batch = [self.dataset[self.indexes[k]][0] for k in range(i, len(self.indexes))]\n",
        "      Y_batch = [self.dataset[self.indexes[k]][1] for k in range(i, len(self.indexes))]\n",
        "      yield torch.stack(X_batch).float(), torch.tensor(Y_batch).long()\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "  \"\"\"\n",
        "  Creates a DataSet from numpy arrays while keeping the data \n",
        "  in the more efficient numpy arrays for as long as possible and only\n",
        "  converting to torchtensors when needed (torch tensors are the objects used\n",
        "  to pass the data through the neural network and apply weights).\n",
        "  \"\"\"\n",
        "  \n",
        "  def __init__(self, x, y, transform=None, target_transform=None):\n",
        "    self.targets = y\n",
        "    self.imgs = x\n",
        "    self.transform = transform\n",
        "    self.target_transform = target_transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.targets)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    image = torch.from_numpy(self.imgs[idx] / 255).float()\n",
        "    label = self.targets[idx]\n",
        "    return image, label\n",
        "\n",
        "def load_numpy_arr_from_url(url):\n",
        "    \"\"\"\n",
        "    Loads a numpy array from surfdrive. \n",
        "    \n",
        "    Input:\n",
        "    url: Download link of dataset \n",
        "    \n",
        "    Outputs:\n",
        "    dataset: numpy array with input features or labels\n",
        "    \"\"\"\n",
        "    \n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    return np.load(io.BytesIO(response.content)) \n",
        "\n",
        "\n",
        "class_labels = {0: 'Atelectasis',\n",
        "                1: 'Effusion',\n",
        "                2: 'Infiltration',\n",
        "                3: 'No Finding',\n",
        "                4: 'Nodule',\n",
        "                5: 'Pneumothorax'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2qLHzgiJ-ta"
      },
      "source": [
        "# Downloading the data:\n",
        "The following cells will download the pre-processed X-ray images with their accompanying labels.\n",
        "\n",
        "The download (400 MB) may take a while."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Q0SUE4M0Jmwv"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# Downloading the labels of each image:\n",
        "train_y = load_numpy_arr_from_url('https://surfdrive.surf.nl/files/index.php/s/i6MvQ8nqoiQ9Tci/download')\n",
        "test_y = load_numpy_arr_from_url('https://surfdrive.surf.nl/files/index.php/s/wLXiOjVAW4AWlXY/download')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFl8GOX_L80K"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# Downloading the images:\n",
        "train_x = load_numpy_arr_from_url('https://surfdrive.surf.nl/files/index.php/s/4rwSf9SYO1ydGtK/download')\n",
        "test_x = load_numpy_arr_from_url('https://surfdrive.surf.nl/files/index.php/s/dvY2LpvFo6dHef0/download')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPvS0ZkfjZdl"
      },
      "source": [
        "### changes on data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QmV9SJnc6vW"
      },
      "outputs": [],
      "source": [
        "def sizedSet(set_x, set_y):\n",
        "    \"\"\"Given a data set x this function makes all the labeled (by y) sub sets\n",
        "    the same size as the smallest subset of x by removing the other points \n",
        "    \"\"\"\n",
        "\n",
        "    set_arr = np.array([[test_x[i],test_y[i]] for i in range(len(test_y))])\n",
        "    set_arr = set_arr[set_arr[:,1].argsort()]\n",
        "    label_sizes = np.array([np.count_nonzero(test_y==i) for i in class_labels])\n",
        "    size = int(min(label_sizes))\n",
        "\n",
        "    new_arr=np.array([])\n",
        "    index = 0 \n",
        "\n",
        "    for l in range(np.size(label_sizes)):\n",
        "        for i in range(size):\n",
        "            new_arr = np.append(new_arr , [set_arr[index+i,0],set_arr[index+i,1]])\n",
        "        index += label_sizes[l]\n",
        "    \n",
        "    new_x=[]\n",
        "    new_y=[]\n",
        "    for i in range(int(np.size(new_arr)/2)):\n",
        "        new_x.append(new_arr[2*i])\n",
        "        new_y.append(new_arr[2*i+1])\n",
        "\n",
        "    new_x = np.array(new_x)\n",
        "    new_y = np.array(new_y)\n",
        "    return new_x, new_y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gaeGIlWjP39"
      },
      "outputs": [],
      "source": [
        "# comment out to use full data set\n",
        "#train_x , train_y = sizedSet(train_x, train_y)\n",
        "#test_x , test_y = sizedSet(test_x, test_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNZDXSGYlioO"
      },
      "source": [
        "# Plotting the data distribution:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PS0HVYPcMaYO"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# Plotting the label distribution in train/test set:\n",
        "fig, ax = plt.subplots(ncols=2, figsize=[20,10])\n",
        "\n",
        "unique, counts = np.unique(train_y, return_counts=True) \n",
        "ax[0].bar([class_labels[i] + f'\\n({c})' for i, c in zip(unique, counts)], counts)\n",
        "ax[0].set_title('Number of images per class in train-set')\n",
        "\n",
        "unique, counts = np.unique(test_y, return_counts=True) \n",
        "ax[1].bar([class_labels[i] + f'\\n({c})' for i, c in zip(unique, counts)], counts)\n",
        "ax[1].set_title('Number of images per class in test-set');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XjlCEzWln29"
      },
      "source": [
        "#Plotting some samples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0R0kB9rgJ88t"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# Plotting some images\n",
        "unique_labels = set(class_labels.keys())\n",
        "fig, ax = plt.subplots(ncols=len(unique_labels), figsize=[30,10])\n",
        "\n",
        "for k, label in enumerate(unique_labels):\n",
        "  ind = list(train_y).index(label)\n",
        "  ax[k].imshow(train_x[ind].reshape(128,128), cmap='gray')\n",
        "  ax[k].set_title(f'Class:{class_labels[train_y[ind]]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwoPlextltT7"
      },
      "source": [
        "# Building torch datasets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-kfDytTP9I95"
      },
      "outputs": [],
      "source": [
        "train_dataset = ImageDataset(train_x, train_y)\n",
        "test_dataset = ImageDataset(test_x, test_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADPO8ctCicHs"
      },
      "source": [
        "# Defining our model as a neural network:\n",
        "**TODO** define your own model here, follow the structure as presented in the Pytorch tutorial (or see below as an example)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TkbIbEU-O-0e"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):   \n",
        "    def __init__(self, n_classes):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.cnn_layers = nn.Sequential(\n",
        "            # Defining a 2D convolution layer\n",
        "            nn.Conv2d(1, 128, kernel_size=4, stride=1, padding=2, dilation=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.RReLU(),\n",
        "            nn.MaxPool2d(kernel_size=4),\n",
        "            torch.nn.Dropout(p=0.5, inplace=True),\n",
        "            # Defining another 2D convolution layer\n",
        "            nn.Conv2d(128, 64, kernel_size=4, stride=1, padding=1, dilation=1, groups=4),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.RReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3),\n",
        "            torch.nn.Dropout(p=0.25, inplace=True),\n",
        "            # Defining another 2D convolution layer\n",
        "            nn.Conv2d(64, 16, kernel_size=4, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.RReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            torch.nn.Dropout(p=0.25, inplace=True),\n",
        "        )\n",
        "\n",
        "        self.linear_layers = nn.Sequential(\n",
        "            nn.Linear(144, 256),\n",
        "            nn.Linear(256,512),\n",
        "            nn.Linear(512, n_classes)\n",
        "        )\n",
        "\n",
        "    # Defining the forward pass    \n",
        "    def forward(self, x):\n",
        "        x = self.cnn_layers(x)\n",
        "        # After our convolutional layers which are 2D, we need to flatten our\n",
        "        # input to be 1 dimensional, as the linear layers require this.\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.linear_layers(x)\n",
        "        return x\n",
        "\n",
        "# Make sure your model instance is assigned to a variable 'model':\n",
        "model = Net(n_classes = 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvkln77gmrOl"
      },
      "source": [
        "#Moving model to CUDA, verifying model structure and printing a summary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5o5m_hEocYY"
      },
      "outputs": [],
      "source": [
        "# IMPORTANT! Set this to True to see actual errors regarding \n",
        "# the structure of your model (CUDA hides them)!\n",
        "# Also make sure you set this to False again for actual model training\n",
        "# as training your model with GPU-acceleration (CUDA) is much faster.\n",
        "DEBUG = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJj3tmq8mnY-"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# Moving our model to the right device (CUDA will speed training up significantly!)\n",
        "if torch.cuda.is_available() and not DEBUG:\n",
        "  device = 'cuda'\n",
        "  print(f\"Device = {device}\")\n",
        "  model.to(device)\n",
        "  # Creating a summary of our model and its layers:\n",
        "  summary(model, (1, 128, 128), device=device)\n",
        "else:\n",
        "  device='cpu'\n",
        "  print(f\"Device = {device}\")\n",
        "  # Creating a summary of our model and its layers:\n",
        "  summary(model, (1, 128, 128), device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1Yjb3uzm_ar"
      },
      "source": [
        "# Defining our loss and optimizer functions:\n",
        "**TODO** Please define your own optimizer and loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gh6efXnHm-SF"
      },
      "outputs": [],
      "source": [
        "label_sizes = np.array([np.count_nonzero(train_y==i) for i in class_labels])\n",
        "label_ratios = [(1-i/sum(label_sizes)) for i in label_sizes]\n",
        "label_ratios_l = [float(i/sum(label_ratios)) for i in label_ratios]\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.0075, momentum=0.125)\n",
        "loss_function = nn.CrossEntropyLoss(weight=torch.tensor(label_ratios_l).to(device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "id1Np1S3nNoh"
      },
      "source": [
        "#Defining our training/testing methods:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iz7CfpfYnM_b"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "def train_model(model, train_sampler, optimizer, loss_function):\n",
        "  # Lets keep track of all the losses:\n",
        "  losses = []\n",
        "  # Put the model in train mode:\n",
        "  model.train()\n",
        "  # Feed all the batches one by one:\n",
        "  for batch in tqdm(train_sampler):\n",
        "    # Get a batch:\n",
        "    x, y = batch\n",
        "    # Making sure our samples are stored on the same device as our model:\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    # Get predictions:\n",
        "    predictions = model.forward(x)\n",
        "    loss = loss_function(predictions, y)\n",
        "    losses.append(loss)\n",
        "    # We first need to make sure we reset our optimizer at the start.\n",
        "    # We want to learn from each batch seperately, \n",
        "    # not from the entire dataset at once.\n",
        "    optimizer.zero_grad()\n",
        "    # We now backpropagate our loss through our model:\n",
        "    loss.backward()\n",
        "    # We then make the optimizer take a step in the right direction.\n",
        "    optimizer.step()\n",
        "  return losses\n",
        "\n",
        "def test_model(model, test_sampler, loss_function):\n",
        "  # Setting the model to evaluation mode:\n",
        "  model.eval()\n",
        "  losses = []\n",
        "  # We need to make sure we do not update our model based on the test data:\n",
        "  with torch.no_grad():\n",
        "    for (x, y) in tqdm(test_sampler):\n",
        "      # Making sure our samples are stored on the same device as our model:\n",
        "      x = x.to(device)\n",
        "      y = y.to(device)\n",
        "      prediction = model.forward(x)\n",
        "      loss = loss_function(prediction, y)\n",
        "      losses.append(loss)\n",
        "  return losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDG8XNh_ojrW"
      },
      "source": [
        "#Training our model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5mPvRt5op0z"
      },
      "outputs": [],
      "source": [
        "n_epochs = 35\n",
        "batch_size = 35"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAunFnrpSDKU"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# Lets now train and test our model for multiple epochs:\n",
        "train_sampler = BatchSampler(batch_size=batch_size, dataset=train_dataset, balanced=False)\n",
        "test_sampler = BatchSampler(batch_size=100, dataset=test_dataset, balanced=False)\n",
        "\n",
        "mean_losses_train = []\n",
        "mean_losses_test = []\n",
        "accuracies = []\n",
        "for e in range(n_epochs):\n",
        "  # Training:\n",
        "  losses = train_model(model, train_sampler, optimizer, loss_function)\n",
        "  # Calculating and printing statistics:\n",
        "  mean_loss = sum(losses) / len(losses)\n",
        "  mean_losses_train.append(mean_loss)\n",
        "  print(f'\\nEpoch {e + 1} training done, loss on train set: {mean_loss}\\n')\n",
        "\n",
        "  # Testing:\n",
        "  losses = test_model(model, test_sampler, loss_function)\n",
        "  # Calculating and printing statistics:\n",
        "  mean_loss = sum(losses) / len(losses)\n",
        "  mean_losses_test.append(mean_loss)\n",
        "  print(f'\\nEpoch {e + 1} testing done, loss on test set: {mean_loss}\\n')\n",
        "  # Plotting the historic loss:\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.plot(mean_losses_train, label='Train loss')\n",
        "  ax.plot(mean_losses_test, label='Test loss')\n",
        "  ax.legend()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tnZQoGJqKND"
      },
      "source": [
        "# Evaluation our model:\n",
        "**TODO** write your own methods to evaluate the model. For example, calculate the accuracy of the model on the test-set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jw8ox5pYqWaD"
      },
      "outputs": [],
      "source": [
        "# Calculate false positives and false negatives from model from the testing set\n",
        "\n",
        "\n",
        "def ConfusionMatrix():\n",
        "    \"\"\" False positives + False Negatives\n",
        "Will the no-finding in a set and the non-no-finding on another\n",
        "    \"\"\"\n",
        "    \n",
        "    # prepare data \n",
        "    testing_arr = np.array([[test_x[i],test_y[i]] for i in range(len(test_y))])\n",
        "    filter_arr = [i[1]==3 for i in testing_arr]\n",
        "    healthy_arr = testing_arr[filter_arr]\n",
        "    filter_arr = [not i for i in filter_arr]\n",
        "    disease_arr = testing_arr[filter_arr]\n",
        "\n",
        "    # prepare sampler data sets \n",
        "    pre_x = [i[0] for i in healthy_arr]\n",
        "    pre_y = [i[1] for i in healthy_arr]\n",
        "    healthy_len = len(pre_y)\n",
        "    test_dataset_healthy = ImageDataset(pre_x,pre_y)\n",
        "    test_sampler_healthy = BatchSampler(batch_size=100, dataset=test_dataset_healthy, balanced=False)\n",
        "\n",
        "    pre_x = [i[0] for i in disease_arr]\n",
        "    pre_y = [i[1] for i in disease_arr]\n",
        "    disease_len = len(pre_y)\n",
        "    test_dataset_disease = ImageDataset(pre_x,pre_y)\n",
        "    test_sampler_disease = BatchSampler(batch_size=100, dataset=test_dataset_disease, balanced=False)\n",
        "\n",
        "\n",
        "    # eval accuracy \n",
        "\n",
        "    model.eval()\n",
        "    # We need to make sure we do not update our model based on the test data:\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        count = 0\n",
        "        for (x, y) in tqdm(test_sampler_disease):\n",
        "        # Making sure our samples are stored on the same device as our model:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            prediction = model.forward(x).argmax(axis=1)\n",
        "            correct += sum(prediction == y)\n",
        "            \n",
        "            count += len(y)\n",
        "        true_pos = (correct).detach().cpu().numpy()\n",
        "        false_pos = (disease_len-correct).detach().cpu().numpy()\n",
        "\n",
        "\n",
        "    # We need to make sure we do not update our model based on the test data:\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        count = 0\n",
        "        for (x, y) in tqdm(test_sampler_healthy):\n",
        "        # Making sure our samples are stored on the same device as our model:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            prediction = model.forward(x).argmax(axis=1)\n",
        "            correct += sum(prediction == y)\n",
        "            \n",
        "            count += len(y)\n",
        "        true_neg = (correct).detach().cpu().numpy()\n",
        "        false_neg = (healthy_len-correct).detach().cpu().numpy()\n",
        "\n",
        "\n",
        "\n",
        "    return true_pos, false_pos, true_neg, false_neg\n",
        "\n",
        "# =======================================>\n",
        "\n",
        "# Evaluation method 0.1.1\n",
        "\n",
        "def evalMethod011():\n",
        "    \"\"\" Evaluation method 0.1.1\n",
        "Takes test set and separates points in 6 sub sets per label in the y component\n",
        "accuracy of the model is taken to each subset by calculating the percentage of \n",
        "points which were classified correctly.\n",
        "A final accuracy is calculating by a linear combination of these sub-acuracies\n",
        "weighted by the size of each sub data set (|n_i|), the higher the size the \n",
        "smaller the weight. \n",
        "    \"\"\"\n",
        "    testing_arr = np.array([[test_x[i],test_y[i]] for i in range(len(test_y))])\n",
        "    testing_arr = testing_arr[testing_arr[:,1].argsort()]\n",
        "    label_sizes = np.array([np.count_nonzero(test_y==i) for i in class_labels])\n",
        "    testing_x = np.array([testing_arr[i,0] for i in range(len(test_y))])\n",
        "    testing_y = np.array([testing_arr[i,1] for i in range(len(test_y))])\n",
        "\n",
        "    model.eval()\n",
        "    # We need to make sure we do not update our model based on the test data:\n",
        "    a=0\n",
        "    label_index=[]\n",
        "    for i in range(len(label_sizes)):\n",
        "        a+=label_sizes[i]\n",
        "        label_index.append(a)\n",
        "\n",
        "    p_index=0\n",
        "\n",
        "    accuracy_l=[]\n",
        "\n",
        "    for l in class_labels: \n",
        "        pre_x=[]\n",
        "        pre_y=[]\n",
        "        for i in range(p_index,label_index[l]):\n",
        "            pre_x.append(testing_x[i])\n",
        "            pre_y.append(testing_y[i])\n",
        "\n",
        "        p_index = label_index[l]   \n",
        "\n",
        "        test_dataset_l = ImageDataset(pre_x,pre_y)\n",
        "        test_sampler_l = BatchSampler(batch_size=100, dataset=test_dataset_l, balanced=False)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            correct = 0\n",
        "            count = 0\n",
        "            for (x, y) in tqdm(test_sampler_l):\n",
        "                # Making sure our samples are stored on the same device as our model:\n",
        "                x = x.to(device)\n",
        "                y = y.to(device)\n",
        "                prediction = model.forward(x).argmax(axis=1)\n",
        "                correct += sum(prediction == y)\n",
        "                count += len(y)\n",
        "        accuracy = (correct/count).detach().cpu().numpy()\n",
        "        accuracy_l.append(accuracy)\n",
        "        print(f'\\nAccuracy of model on test set on label {class_labels[l]}: {accuracy}')\n",
        "\n",
        "    accuracy_l=[i for i in accuracy_l]\n",
        "    label_ratios = [(1-i/sum(label_sizes)) for i in label_sizes]\n",
        "    label_ratios_l = [i/sum(label_ratios) for i in label_ratios]\n",
        "    final_accuracy=0\n",
        "    for i in range(6):\n",
        "        final_accuracy+= accuracy_l[i]*label_ratios_l[i]\n",
        "    print(f\"The general accurcy of the model is: {final_accuracy}\")\n",
        "    print(\"The smallest accuracy necessary for the model not to be useless is 0.5\")\n",
        "\n",
        "# ==========================================>\n",
        "\n",
        "# Evaluation method 0.1.0\n",
        "\n",
        "def evalMethod010():\n",
        "    \"\"\" Evaluation method 0.1.0\n",
        "Evaluaion method given in template. Simply takes the percentage of correctly\n",
        "points from the model. Nothing else is taken into account\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    # We need to make sure we do not update our model based on the test data:\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        count = 0\n",
        "        for (x, y) in tqdm(test_sampler):\n",
        "        # Making sure our samples are stored on the same device as our model:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            prediction = model.forward(x).argmax(axis=1)\n",
        "            correct += sum(prediction == y)\n",
        "            count += len(y)\n",
        "        accuracy = (correct/count).detach().cpu().numpy()\n",
        "        print(f'\\nAccuracy of model on test set: {accuracy}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_gmbaXr42VA"
      },
      "outputs": [],
      "source": [
        "TP, FP, TN, FN = ConfusionMatrix()\n",
        "\n",
        "Precision = TP/(TP+FP)\n",
        "Recall = TP/(TP+FN)\n",
        "print(\"\\n\")\n",
        "print(f\"TP = {TP}\")\n",
        "print(f\"TN = {TN}\")\n",
        "print(f\"FP = {FP}\")\n",
        "print(f\"FN = {FN}\")\n",
        "print(f\"Precision = {Precision}\")\n",
        "print(f\"Recall = {Recall}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mj2py6-v8iXh"
      },
      "outputs": [],
      "source": [
        "evalMethod011()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3nM958rnYPH"
      },
      "source": [
        "This accuracy isn't great. Your task is to find a better model that performs better at the classification task. Other methods of evaluation might tell you more why a particular model is not performing well (accuracy is a quite limited aggregated performance metric). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-O_nBMQSfgl"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1fo7K_1mq1L"
      },
      "source": [
        "# Saving our model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esVBn4-N1ij9"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "torch.save(model.state_dict(), '/content/gdrive/My Drive/weights_model.txt')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Data_Challenge_1_Model_(03_0_1_4).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}