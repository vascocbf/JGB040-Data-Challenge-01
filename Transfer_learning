# Import model
model_inception = torch.hub.load('pytorch/vision:v0.10.0', 'inception_v3',
                                 pretrained=True)

num_ftrs = model_inception.fc.in_features
model_inception.fc = nn.Linear(num_ftrs, 6)

model_inception = model_inception.to(device)

n_epochs = 10
batch_size = 25

# call to normalize data
preprocess = transforms.Compose([
    #preprocess takes tensor x
    transforms.Lambda(lambda x: x.repeat(1, 3, 1, 1)),
    transforms.Resize(299),
    transforms.CenterCrop(299),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

train_sampler = BatchSampler(
    batch_size=batch_size,
    dataset=train_dataset,
    balanced=False)

test_sampler = BatchSampler(
    batch_size=batch_size,
    dataset=train_dataset,
    balanced=False,
    )

# train function -- in progress
def train_model1(model, train_sampler, optimizer, loss_function):
  losses = []
  model.train()
  model.to('cuda')  

  for (x, y) in tqdm(train_sampler):
    x = preprocess(x.to('cuda'))
    y = y.to('cuda')
    optimizer.zero_grad()

    predictions = model.forward(x) 
    probabilities = torch.nn.functional.softmax(predictions[0], dim=0)
    
    loss = loss_function(probabilities, y)
    losses.append(loss)
    
    loss.backward()
    optimizer.step()
  return losses
  
# test function -- in progress
def test_model1(model, test_sampler, loss_function):
  model.eval()
  losses = []
  
  with torch.no_grad():
    for (x, y) in tqdm(test_sampler):
      x = preprocess(x.to('cuda'))
      y = y.to('cuda')
      y = y.float()

      prediction = model.forward(x)
      probability = torch.nn.functional.softmax(prediction[0], dim=0)
      print(y.type())

      loss = loss_function(probability[0], y[0])
      losses.append(loss)
    
  return losses
